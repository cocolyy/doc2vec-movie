{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from scipy.spatial import distance\n",
    "import re\n",
    "from random import shuffle\n",
    "import string\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import pickle\n",
    "import pprint\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv('dataset/movies.csv', index_col=0) \n",
    "df_final.dropna(inplace=True)\n",
    "df_final.reset_index(drop=True, inplace=True)\n",
    "df_final['Description'] = df_final['Description'].apply(lambda x: str(x).strip())\n",
    "\n",
    "def make_lower_case(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    text = text.split()\n",
    "    stops = set(stopwords.words(\"turkish\"))\n",
    "    text = [w for w in text if not w in stops]\n",
    "    text = \" \".join(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    new_string = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return new_string\n",
    "\n",
    "# Description only\n",
    "df_final['cleaned'] = df_final['Description']\n",
    "# Stars + Directors + Description\n",
    "df_final = df_final.assign(combine=[(f'''{str(df_final[\"Title\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")} ,\n",
    "{str(df_final[\"Genres\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")} ,\n",
    "{str(df_final[\"Stars\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")} ,\n",
    "{str(df_final[\"Stars\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")} ,\n",
    "{str(df_final[\"Director\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")},\n",
    "{str(df_final[\"Genres\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")},\n",
    "{str(df_final[\"Genres\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")}''').lower() for i in range(len(df_final[\"Title\"]))])\n",
    "# Title + Description\n",
    "df_final = df_final.assign(temp_f=[(f'''{str(df_final[\"Stars\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")}, {str(df_final[\"Director\"][i]).strip(\"[]\").replace(\"'\", \"\").replace(\",\",\".\")}, {df_final['Description']}''').lower() for i in range(len(df_final[\"Title\"]))])\n",
    "\n",
    "df_final['cleaned'] = df_final.cleaned.apply(func = make_lower_case)\n",
    "df_final['cleaned'] = df_final.cleaned.apply(func = remove_stop_words)\n",
    "df_final['cleaned'] = df_final.cleaned.apply(func = remove_punctuation)\n",
    "df_final['cleaned'] = df_final.cleaned.apply(func = remove_html)\n",
    "\n",
    "df_final['combine'] = df_final['combine'].apply(func = make_lower_case)\n",
    "df_final['combine'] = df_final['combine'].apply(func = remove_stop_words)\n",
    "df_final['combine'] = df_final['combine'].apply(func = remove_punctuation)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(row['combine']), tags=[row['Title']]) for index, row in df_final.iterrows()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yüzüklerin Efendisi: İki Kule'] :  ['yüzüklerin', 'efendisi', 'i̇ki', 'kule', 'fantastik', 'macera', 'elijah', 'wood', 'sean', 'astin', 'viggo', 'mortensen', 'elijah', 'wood', 'sean', 'astin', 'viggo', 'mortensen', 'peter', 'jackson', 'fantastik', 'macera', 'fantastik', 'macera']\n"
     ]
    }
   ],
   "source": [
    "print(tagged_data[471].tags, ': ', (tagged_data[471].words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 18:30:25,498 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t12>', 'datetime': '2023-02-22T18:30:25.498286', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-02-22 18:30:25,499 : INFO : collecting all words and their counts\n",
      "2023-02-22 18:30:25,499 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2023-02-22 18:30:25,537 : INFO : PROGRESS: at example #10000, processed 225567 words (5892077 words/s), 26085 word types, 9472 tags\n",
      "2023-02-22 18:30:25,621 : INFO : collected 42687 word types and 17513 unique tags from a corpus of 18492 examples and 414804 words\n",
      "2023-02-22 18:30:25,622 : INFO : Creating a fresh vocabulary\n",
      "2023-02-22 18:30:25,650 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 10246 unique words (24.00% of original 42687, drops 32441)', 'datetime': '2023-02-22T18:30:25.650462', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-22 18:30:25,651 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 353881 word corpus (85.31% of original 414804, drops 60923)', 'datetime': '2023-02-22T18:30:25.651464', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-22 18:30:25,685 : INFO : deleting the raw counts dictionary of 42687 items\n",
      "2023-02-22 18:30:25,686 : INFO : sample=0.001 downsamples 24 most-common words\n",
      "2023-02-22 18:30:25,686 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 278835.0016274331 word corpus (78.8%% of prior 353881)', 'datetime': '2023-02-22T18:30:25.686997', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-22 18:30:25,745 : INFO : estimated required memory for 10246 words and 200 dimensions: 39029600 bytes\n",
      "2023-02-22 18:30:25,745 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t12>\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(\n",
    "    dm=0, dbow_words=1,\n",
    "    vector_size=200, window=8, epochs=20, workers=12,\n",
    ")\n",
    "\n",
    "model_dbow.build_vocab(tagged_data)\n",
    "print(model_dbow)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 18:30:25,803 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 12 workers on 10246 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-22T18:30:25.803501', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-02-22 18:30:26,988 : INFO : EPOCH 0 - PROGRESS: at 59.90% examples, 156785 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:27,128 : INFO : EPOCH 0: training on 414804 raw words (297418 effective words) took 1.3s, 226331 effective words/s\n",
      "2023-02-22 18:30:28,337 : INFO : EPOCH 1 - PROGRESS: at 59.90% examples, 153212 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:28,485 : INFO : EPOCH 1: training on 414804 raw words (297129 effective words) took 1.3s, 220399 effective words/s\n",
      "2023-02-22 18:30:29,735 : INFO : EPOCH 2 - PROGRESS: at 59.90% examples, 148354 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:29,904 : INFO : EPOCH 2: training on 414804 raw words (297280 effective words) took 1.4s, 210955 effective words/s\n",
      "2023-02-22 18:30:31,145 : INFO : EPOCH 3 - PROGRESS: at 59.90% examples, 150181 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:31,283 : INFO : EPOCH 3: training on 414804 raw words (297169 effective words) took 1.4s, 218107 effective words/s\n",
      "2023-02-22 18:30:32,527 : INFO : EPOCH 4 - PROGRESS: at 59.90% examples, 149138 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:32,670 : INFO : EPOCH 4: training on 414804 raw words (297310 effective words) took 1.4s, 216022 effective words/s\n",
      "2023-02-22 18:30:33,915 : INFO : EPOCH 5 - PROGRESS: at 59.90% examples, 148880 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:34,068 : INFO : EPOCH 5: training on 414804 raw words (297350 effective words) took 1.4s, 214218 effective words/s\n",
      "2023-02-22 18:30:35,304 : INFO : EPOCH 6 - PROGRESS: at 59.90% examples, 149697 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:35,448 : INFO : EPOCH 6: training on 414804 raw words (297038 effective words) took 1.4s, 216655 effective words/s\n",
      "2023-02-22 18:30:36,689 : INFO : EPOCH 7 - PROGRESS: at 59.90% examples, 149458 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:36,833 : INFO : EPOCH 7: training on 414804 raw words (297307 effective words) took 1.4s, 216029 effective words/s\n",
      "2023-02-22 18:30:38,053 : INFO : EPOCH 8 - PROGRESS: at 59.90% examples, 152022 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:38,192 : INFO : EPOCH 8: training on 414804 raw words (297362 effective words) took 1.4s, 220161 effective words/s\n",
      "2023-02-22 18:30:39,421 : INFO : EPOCH 9 - PROGRESS: at 59.93% examples, 151418 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:39,561 : INFO : EPOCH 9: training on 414804 raw words (297435 effective words) took 1.4s, 219537 effective words/s\n",
      "2023-02-22 18:30:40,776 : INFO : EPOCH 10 - PROGRESS: at 59.90% examples, 152500 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:40,921 : INFO : EPOCH 10: training on 414804 raw words (297333 effective words) took 1.4s, 220181 effective words/s\n",
      "2023-02-22 18:30:42,159 : INFO : EPOCH 11 - PROGRESS: at 59.90% examples, 149873 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:42,309 : INFO : EPOCH 11: training on 414804 raw words (297470 effective words) took 1.4s, 215606 effective words/s\n",
      "2023-02-22 18:30:43,515 : INFO : EPOCH 12 - PROGRESS: at 59.95% examples, 153798 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:43,670 : INFO : EPOCH 12: training on 414804 raw words (297208 effective words) took 1.4s, 220017 effective words/s\n",
      "2023-02-22 18:30:44,879 : INFO : EPOCH 13 - PROGRESS: at 59.90% examples, 153385 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:45,027 : INFO : EPOCH 13: training on 414804 raw words (297383 effective words) took 1.3s, 220533 effective words/s\n",
      "2023-02-22 18:30:46,238 : INFO : EPOCH 14 - PROGRESS: at 59.95% examples, 153236 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:46,387 : INFO : EPOCH 14: training on 414804 raw words (297460 effective words) took 1.4s, 220286 effective words/s\n",
      "2023-02-22 18:30:47,607 : INFO : EPOCH 15 - PROGRESS: at 59.90% examples, 152167 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:47,745 : INFO : EPOCH 15: training on 414804 raw words (297425 effective words) took 1.3s, 220672 effective words/s\n",
      "2023-02-22 18:30:48,969 : INFO : EPOCH 16 - PROGRESS: at 59.90% examples, 152112 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:49,112 : INFO : EPOCH 16: training on 414804 raw words (297536 effective words) took 1.4s, 219831 effective words/s\n",
      "2023-02-22 18:30:50,339 : INFO : EPOCH 17 - PROGRESS: at 59.93% examples, 150942 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:50,483 : INFO : EPOCH 17: training on 414804 raw words (297239 effective words) took 1.4s, 218231 effective words/s\n",
      "2023-02-22 18:30:51,700 : INFO : EPOCH 18 - PROGRESS: at 59.90% examples, 152424 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:51,849 : INFO : EPOCH 18: training on 414804 raw words (297432 effective words) took 1.4s, 219160 effective words/s\n",
      "2023-02-22 18:30:53,066 : INFO : EPOCH 19 - PROGRESS: at 59.90% examples, 152407 words/s, in_qsize 17, out_qsize 0\n",
      "2023-02-22 18:30:53,211 : INFO : EPOCH 19: training on 414804 raw words (297490 effective words) took 1.4s, 219860 effective words/s\n",
      "2023-02-22 18:30:53,212 : INFO : Doc2Vec lifecycle event {'msg': 'training on 8296080 raw words (5946774 effective words) took 27.4s, 216977 effective words/s', 'datetime': '2023-02-22T18:30:53.212637', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-02-22 18:30:53,212 : INFO : Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-22T18:30:53.212637', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'saving'}\n",
      "2023-02-22 18:30:53,213 : INFO : not storing attribute cum_table\n",
      "2023-02-22 18:30:53,237 : INFO : saved doc2vec.model\n"
     ]
    }
   ],
   "source": [
    "model_dbow.train(tagged_data, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)\n",
    "model_dbow.save(\"doc2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Iron Man', 0.9398001432418823),\n",
      "    ('Iron Man 3', 0.8286119699478149),\n",
      "    ('Iron Man 2', 0.8158987760543823),\n",
      "    ('Yenilmezler', 0.7131946086883545),\n",
      "    ('Red Tails', 0.7069835066795349),\n",
      "    ('Sky Captain ve Yarının Dünyası', 0.6855823993682861),\n",
      "    ('Avengers: Endgame', 0.6840946674346924),\n",
      "    ('Yenilmezler: Ultron Çağı', 0.6807721853256226),\n",
      "    ('Avengers: Sonsuzluk Savaşı', 0.6733619570732117),\n",
      "    ('Jurassic World: Hakimiyet', 0.6591430306434631),\n",
      "    ('Air America', 0.6523217558860779),\n",
      "    ('Sherlock Holmes', 0.6520013213157654),\n",
      "    ('Örümcek-Adam: Eve Dönüş', 0.6508820652961731),\n",
      "    ('Yüksek Gerilim', 0.6480500102043152),\n",
      "    ('Gothika', 0.6461778879165649),\n",
      "    ('Dövüş', 0.645106315612793),\n",
      "    ('Fur: An Imaginary Portrait of Diane Arbus', 0.6416605710983276),\n",
      "    ('A Scanner Darkly', 0.639180600643158),\n",
      "    ('Tuff Turf', 0.638505220413208),\n",
      "    ('Kanıt', 0.6371035575866699)]\n",
      "iron man aksiyon bilimkurgu robert downey jr terrence howard gwyneth paltrow robert downey jr terrence howard gwyneth paltrow jon favreau aksiyon bilimkurgu aksiyon bilimkurgu\n"
     ]
    }
   ],
   "source": [
    "def search_by_tag(tag):\n",
    "    for doc in tagged_data:\n",
    "        if doc.tags[0] == tag:\n",
    "            return \" \".join(doc.words)\n",
    "\n",
    "def return_most_similar(mov_title) -> list:\n",
    "  tokens = remove_html(remove_punctuation(remove_stop_words(make_lower_case(search_by_tag(mov_title))))).split()\n",
    "  vector = model_dbow.infer_vector(tokens)\n",
    "  most_similar = model_dbow.dv.most_similar([vector], topn=20) \n",
    "  return most_similar\n",
    "\n",
    "def return_token(mov_title) -> list:\n",
    "  tokens = remove_html(remove_punctuation(remove_stop_words(make_lower_case(search_by_tag(mov_title))))).split()\n",
    "  return tokens\n",
    "\n",
    "def return_most_similar_token(vec) -> list:\n",
    "  # len(df_final['Title']\n",
    "  most_similar = model_dbow.dv.most_similar([vec],topn=20)\n",
    "  return most_similar\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "seed_text = 'Iron Man'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)\n",
    "print(search_by_tag(seed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Dune:Çöl Gezegeni', 0.9543393850326538),\n",
      "    ('Dune: Part Two', 0.9015074372291565),\n",
      "    ('Hayat', 0.718077540397644),\n",
      "    ('Ex Machina', 0.6980426907539368),\n",
      "    ('Metal Gear Solid', 0.6805034279823303),\n",
      "    ('Trendeki Kız', 0.6732518076896667),\n",
      "    ('Star Wars: Son Jedi', 0.6704469323158264),\n",
      "    ('Kardan Adam', 0.6681733727455139),\n",
      "    (\"Star Wars: Skywalker'ın Yükselişi\", 0.6627540588378906),\n",
      "    (\"Won't Back Down\", 0.6611697673797607),\n",
      "    ('Mojave', 0.6510500311851501),\n",
      "    ('Her Şey Olacağına Varır', 0.649515688419342),\n",
      "    ('Zihin Gezgini', 0.647402822971344),\n",
      "    ('Zehirli Element', 0.6454704403877258),\n",
      "    ('Doktor Uyku', 0.638755202293396),\n",
      "    ('Francis And The Godfather', 0.6355670094490051),\n",
      "    ('London', 0.6329144239425659),\n",
      "    ('Beni Adınla Çağır', 0.6269508004188538),\n",
      "    ('Teen Spirit', 0.6261535882949829),\n",
      "    ('Ocak Ayının İki Yüzü', 0.6254170536994934)]\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Dune:Çöl Gezegeni'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Yüzüklerin Efendisi: İki Kule', 0.9456788301467896),\n",
      "    ('Yüzüklerin Efendisi: Yüzük Kardeşliği', 0.8622255921363831),\n",
      "    ('On Üç Yaşam', 0.6091465950012207),\n",
      "    ('Kurbağa Prens', 0.6070922613143921),\n",
      "    ('Woodlawn', 0.5981312990188599),\n",
      "    ('The Goonies', 0.5964442491531372),\n",
      "    ('Şiddetin Tarihçesi', 0.5905812382698059),\n",
      "    ('Rumpelstiltskin', 0.5901864171028137),\n",
      "    ('Tuhaf Bir Sihir', 0.5881508588790894),\n",
      "    ('Try Seventeen', 0.5879155993461609),\n",
      "    ('Yeşil Rehber', 0.5850781798362732),\n",
      "    ('Yeşil Sokak Holiganları', 0.5835225582122803),\n",
      "    ('Hero Mode', 0.5830981135368347),\n",
      "    ('Time Bandits', 0.5801665186882019),\n",
      "    ('Kanun Benim', 0.579239010810852),\n",
      "    ('Düşüş', 0.5744485259056091),\n",
      "    ('Offence, The', 0.5720624327659607),\n",
      "    ('Şark Vaatleri', 0.568449854850769),\n",
      "    ('Gerçeğin İki Yüzü', 0.5681962966918945),\n",
      "    ('Pitch Perfect', 0.5618053674697876)]\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Yüzüklerin Efendisi: İki Kule'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Rashômon', 0.9667608141899109),\n",
      "    ('Yüksek ve Alçak', 0.886763334274292),\n",
      "    ('Yojimbo', 0.8776783347129822),\n",
      "    ('Sanjuro', 0.8773446679115295),\n",
      "    ('Kanlı Taht', 0.8740440607070923),\n",
      "    ('Ran', 0.8683363795280457),\n",
      "    ('Akahige', 0.8680164217948914),\n",
      "    ('Dava Vekili', 0.8570104837417603),\n",
      "    ('Yedi Samuray', 0.8538091778755188),\n",
      "    ('Akasen Chitai', 0.8534561991691589),\n",
      "    ('Floating Weeds', 0.8412061333656311),\n",
      "    ('Ichimei', 0.8389753699302673),\n",
      "    ('Tokyo Sonatı', 0.8376791477203369),\n",
      "    ('Gizli Kale', 0.8375594615936279),\n",
      "    ('37 Seconds', 0.8366216421127319),\n",
      "    ('Ugetsu', 0.8340597748756409),\n",
      "    ('Geç Sonbahar', 0.8333784341812134),\n",
      "    ('Cold Fish', 0.8316842913627625),\n",
      "    ('Cut', 0.8286798596382141),\n",
      "    ('Chikamatsu monogatari', 0.8266655206680298)]\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Rashômon'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Titanik', 0.9382817149162292),\n",
      "    ('Şöhrete İlk Adım', 0.7013309597969055),\n",
      "    ('Fake !', 0.6686000227928162),\n",
      "    ('The Wager', 0.660822868347168),\n",
      "    ('The Black Hand', 0.6597444415092468),\n",
      "    ('Boşanma', 0.6544262170791626),\n",
      "    ('Bu Çocuğun Hayatı', 0.6543473601341248),\n",
      "    ('Enigma', 0.6496071219444275),\n",
      "    ('Tutku Oyunları', 0.6491042971611023),\n",
      "    ('Tatil', 0.6438833475112915),\n",
      "    ('Hayallerin Peşinde', 0.6434882283210754),\n",
      "    ('Orlando', 0.640713632106781),\n",
      "    ('Roosevelt', 0.6402652263641357),\n",
      "    ('Iris', 0.6394973993301392),\n",
      "    (\"Don's Plum\", 0.6316646933555603),\n",
      "    ('Bu Dünya ve Ötesi', 0.6299759745597839),\n",
      "    ('Kutsal Duman', 0.6259361505508423),\n",
      "    ('Okuyucu', 0.6205527782440186),\n",
      "    ('Amsterdam Ekspres', 0.6186784505844116),\n",
      "    ('127 Saat', 0.6186205744743347)]\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Titanik'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dict has title:embedding -> embeddings are created using that specific movie's plot\n",
    "movie_embeddings = {title: model_dbow.infer_vector(return_token(title)) for title in df_final['Title']}\n",
    "\n",
    "with open(\"movie_embed_desc.pickle\", \"wb\") as f:\n",
    "    pickle.dump(movie_embeddings, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=word_tokenize(row['cleaned']), tags=[row['Title']]) for index, row in df_final.iterrows()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Yüzüklerin Efendisi: İki Kule'] :  ['yüzüklerin', 'efendisi', 'i̇ki', 'kulede', 'yüzük', 'kardeşliği', 'üyelerinin', 'birinin', 'kardeşlik', 'bozulduktan', 'sonra', 'başlarına', 'gelenler', 'anlatılıyor', 'kahramanlarımız', 'gruplar', 'halinde', 'orta', 'dünya', '’', 'nın', 'tehlikeli', 'yerlerinde', 'maceralar', 'yaşayacaklar', 'yeni', 'kavimler', 'çoktan', 'unutulmuş', 'medeniyetlerle', 'tanışacaklarfrodo', 'sam', 'yanlarında', 'zorunlu', 'işbirliği', 'yapacakları', 'eski', 'bir', 'dost', 'olduğu', 'halde', 'tek', 'yüzük', '’', 'ü', 'düşmanın', 'tam', 'kalbine', 'götürmeye', 'çalışırken', 'diğer', 'hobbitler', 'urukhai', '’', 'nin', 'elinden', 'kurtulabilecek', 'mi', 'karanlık', 'tarafa', 'geçmiş', 'olan', 'saruman', '’', 'ın', 'yaptıkları', 'yanına', 'kalacak', 'gandalf', 'olmadan', 'kahramanlarımızın', 'başarılı', 'olma', 'şansı', 'ne', 'büyük', 'karanlığın', 'gelişi', 'yüzük', 'savaşı', '’', 'na', 'dek', 'olanların', 'anlatılacağı', 'i̇ki', 'kule', 'kuşkusuz', 'üçlemenin', 'heyecanlı', 'bölümlerinden', 'birini', 'oluşturuyorjrr', 'tolkien', '’', 'in', '3', 'kitaplık', 'ölümsüz', 'eserinden', 'uyarlanarak', 'bir', 'yılı', 'aşkın', 'süren', 'çekimler', 'sonucu', 'bir', 'arada', 'çekilen', '3', 'filmden', 'ikincisi', 'olan', 'i̇ki', 'kule', '2002', 'aralık', 'ayında', 'vizyona', 'girmek', 'üzere', 'montajlanıyor', 'son', 'rötuşları', 'yapılıyor']\n"
     ]
    }
   ],
   "source": [
    "print(tagged_data[471].tags, ': ', (tagged_data[471].words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 18:31:18,282 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t12>', 'datetime': '2023-02-22T18:31:18.282417', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-02-22 18:31:18,285 : INFO : collecting all words and their counts\n",
      "2023-02-22 18:31:18,285 : INFO : PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2023-02-22 18:31:18,427 : INFO : PROGRESS: at example #10000, processed 823740 words (5833100 words/s), 95690 word types, 9472 tags\n",
      "2023-02-22 18:31:18,570 : INFO : collected 126855 word types and 17513 unique tags from a corpus of 18492 examples and 1339673 words\n",
      "2023-02-22 18:31:18,570 : INFO : Creating a fresh vocabulary\n",
      "2023-02-22 18:31:18,644 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 26434 unique words (20.84% of original 126855, drops 100421)', 'datetime': '2023-02-22T18:31:18.644621', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-22 18:31:18,644 : INFO : Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1187186 word corpus (88.62% of original 1339673, drops 152487)', 'datetime': '2023-02-22T18:31:18.644621', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-22 18:31:18,732 : INFO : deleting the raw counts dictionary of 126855 items\n",
      "2023-02-22 18:31:18,734 : INFO : sample=0.001 downsamples 15 most-common words\n",
      "2023-02-22 18:31:18,735 : INFO : Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 1096762.473020063 word corpus (92.4%% of prior 1187186)', 'datetime': '2023-02-22T18:31:18.735706', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-02-22 18:31:18,883 : INFO : estimated required memory for 26434 words and 200 dimensions: 73024400 bytes\n",
      "2023-02-22 18:31:18,884 : INFO : resetting layer weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec<dbow+w,d200,n5,w8,mc5,s0.001,t12>\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(\n",
    "    dm=0, dbow_words=1,\n",
    "    vector_size=200, window=8, epochs=20, workers=12,\n",
    ")\n",
    "\n",
    "model_dbow.build_vocab(tagged_data)\n",
    "print(model_dbow)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-22 18:31:18,929 : INFO : Doc2Vec lifecycle event {'msg': 'training model with 12 workers on 26434 vocabulary and 200 features, using sg=1 hs=0 sample=0.001 negative=5 window=8 shrink_windows=True', 'datetime': '2023-02-22T18:31:18.929147', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-02-22 18:31:20,064 : INFO : EPOCH 0 - PROGRESS: at 46.21% examples, 534037 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:21,044 : INFO : EPOCH 0: training on 1339673 raw words (1115014 effective words) took 2.1s, 528532 effective words/s\n",
      "2023-02-22 18:31:22,068 : INFO : EPOCH 1 - PROGRESS: at 36.82% examples, 495621 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:23,084 : INFO : EPOCH 1 - PROGRESS: at 87.14% examples, 492978 words/s, in_qsize 14, out_qsize 0\n",
      "2023-02-22 18:31:23,214 : INFO : EPOCH 1: training on 1339673 raw words (1115483 effective words) took 2.2s, 515330 effective words/s\n",
      "2023-02-22 18:31:24,332 : INFO : EPOCH 2 - PROGRESS: at 46.25% examples, 542998 words/s, in_qsize 24, out_qsize 0\n",
      "2023-02-22 18:31:25,210 : INFO : EPOCH 2: training on 1339673 raw words (1115326 effective words) took 2.0s, 560206 effective words/s\n",
      "2023-02-22 18:31:26,315 : INFO : EPOCH 3 - PROGRESS: at 46.25% examples, 548811 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:27,215 : INFO : EPOCH 3: training on 1339673 raw words (1115185 effective words) took 2.0s, 557816 effective words/s\n",
      "2023-02-22 18:31:28,334 : INFO : EPOCH 4 - PROGRESS: at 46.25% examples, 541890 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:29,204 : INFO : EPOCH 4: training on 1339673 raw words (1115443 effective words) took 2.0s, 561997 effective words/s\n",
      "2023-02-22 18:31:30,277 : INFO : EPOCH 5 - PROGRESS: at 46.25% examples, 565605 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:31,131 : INFO : EPOCH 5: training on 1339673 raw words (1115454 effective words) took 1.9s, 580557 effective words/s\n",
      "2023-02-22 18:31:32,222 : INFO : EPOCH 6 - PROGRESS: at 46.21% examples, 555731 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:33,081 : INFO : EPOCH 6: training on 1339673 raw words (1115437 effective words) took 1.9s, 573224 effective words/s\n",
      "2023-02-22 18:31:34,195 : INFO : EPOCH 7 - PROGRESS: at 46.25% examples, 544857 words/s, in_qsize 24, out_qsize 0\n",
      "2023-02-22 18:31:35,036 : INFO : EPOCH 7: training on 1339673 raw words (1115115 effective words) took 2.0s, 571763 effective words/s\n",
      "2023-02-22 18:31:36,100 : INFO : EPOCH 8 - PROGRESS: at 46.25% examples, 570792 words/s, in_qsize 24, out_qsize 0\n",
      "2023-02-22 18:31:36,954 : INFO : EPOCH 8: training on 1339673 raw words (1115319 effective words) took 1.9s, 583343 effective words/s\n",
      "2023-02-22 18:31:38,060 : INFO : EPOCH 9 - PROGRESS: at 46.19% examples, 547755 words/s, in_qsize 24, out_qsize 0\n",
      "2023-02-22 18:31:38,955 : INFO : EPOCH 9: training on 1339673 raw words (1115068 effective words) took 2.0s, 558391 effective words/s\n",
      "2023-02-22 18:31:40,038 : INFO : EPOCH 10 - PROGRESS: at 46.25% examples, 560749 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:40,895 : INFO : EPOCH 10: training on 1339673 raw words (1115403 effective words) took 1.9s, 576892 effective words/s\n",
      "2023-02-22 18:31:41,956 : INFO : EPOCH 11 - PROGRESS: at 46.25% examples, 571383 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:42,832 : INFO : EPOCH 11: training on 1339673 raw words (1115318 effective words) took 1.9s, 577108 effective words/s\n",
      "2023-02-22 18:31:43,932 : INFO : EPOCH 12 - PROGRESS: at 46.25% examples, 551421 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:44,821 : INFO : EPOCH 12: training on 1339673 raw words (1115293 effective words) took 2.0s, 562174 effective words/s\n",
      "2023-02-22 18:31:45,911 : INFO : EPOCH 13 - PROGRESS: at 46.25% examples, 557371 words/s, in_qsize 24, out_qsize 0\n",
      "2023-02-22 18:31:46,787 : INFO : EPOCH 13: training on 1339673 raw words (1115223 effective words) took 2.0s, 569514 effective words/s\n",
      "2023-02-22 18:31:47,850 : INFO : EPOCH 14 - PROGRESS: at 46.25% examples, 571367 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:48,722 : INFO : EPOCH 14: training on 1339673 raw words (1115187 effective words) took 1.9s, 578195 effective words/s\n",
      "2023-02-22 18:31:49,857 : INFO : EPOCH 15 - PROGRESS: at 46.25% examples, 534549 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:50,716 : INFO : EPOCH 15: training on 1339673 raw words (1115278 effective words) took 2.0s, 560700 effective words/s\n",
      "2023-02-22 18:31:51,790 : INFO : EPOCH 16 - PROGRESS: at 46.25% examples, 564689 words/s, in_qsize 24, out_qsize 0\n",
      "2023-02-22 18:31:52,629 : INFO : EPOCH 16: training on 1339673 raw words (1115276 effective words) took 1.9s, 584717 effective words/s\n",
      "2023-02-22 18:31:53,691 : INFO : EPOCH 17 - PROGRESS: at 46.25% examples, 570902 words/s, in_qsize 24, out_qsize 0\n",
      "2023-02-22 18:31:54,571 : INFO : EPOCH 17: training on 1339673 raw words (1115157 effective words) took 1.9s, 575671 effective words/s\n",
      "2023-02-22 18:31:55,674 : INFO : EPOCH 18 - PROGRESS: at 46.25% examples, 549353 words/s, in_qsize 24, out_qsize 0\n",
      "2023-02-22 18:31:56,527 : INFO : EPOCH 18: training on 1339673 raw words (1115213 effective words) took 2.0s, 571581 effective words/s\n",
      "2023-02-22 18:31:57,598 : INFO : EPOCH 19 - PROGRESS: at 46.25% examples, 566421 words/s, in_qsize 23, out_qsize 0\n",
      "2023-02-22 18:31:58,468 : INFO : EPOCH 19: training on 1339673 raw words (1115354 effective words) took 1.9s, 575773 effective words/s\n",
      "2023-02-22 18:31:58,469 : INFO : Doc2Vec lifecycle event {'msg': 'training on 26793460 raw words (22305546 effective words) took 39.5s, 564128 effective words/s', 'datetime': '2023-02-22T18:31:58.469959', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-02-22 18:31:58,469 : INFO : Doc2Vec lifecycle event {'fname_or_handle': 'doc2vec.model', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-02-22T18:31:58.469959', 'gensim': '4.3.0', 'python': '3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'saving'}\n",
      "2023-02-22 18:31:58,471 : INFO : not storing attribute cum_table\n",
      "2023-02-22 18:31:58,514 : INFO : saved doc2vec.model\n"
     ]
    }
   ],
   "source": [
    "model_dbow.train(tagged_data, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)\n",
    "model_dbow.save(\"doc2vec.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Iron Man', 0.9322595000267029),\n",
      "    ('Superlópez', 0.4410777688026428),\n",
      "    (\"Oflu Hoca Trakya'da\", 0.4244818687438965),\n",
      "    ('Rounding Third', 0.4170389473438263),\n",
      "    ('Kickboxer: Vengeance', 0.40591973066329956),\n",
      "    ('Xin Shen Bang: Ne Zha Chongsheng', 0.40542253851890564),\n",
      "    ('Tron', 0.4036935865879059),\n",
      "    ('Ölümsüzlerin Savaşı', 0.40248310565948486),\n",
      "    ('Kontroll', 0.40003448724746704),\n",
      "    ('Ani Ölüm', 0.39879336953163147),\n",
      "    ('Operation Seawolf', 0.3970690667629242),\n",
      "    ('Avengers Confidential: Black Widow & Punisher', 0.39299845695495605),\n",
      "    ('Hard Kill', 0.3912982940673828),\n",
      "    ('Hava Kuvvetleri Bir', 0.39031556248664856),\n",
      "    ('Sessiz Düşman', 0.388791561126709),\n",
      "    (\"Entebbe'de 7 Gün\", 0.3879695534706116),\n",
      "    ('Vexille', 0.38727155327796936),\n",
      "    ('Akıllı Ol', 0.38670727610588074),\n",
      "    ('Kayıp Savaş', 0.3865184187889099),\n",
      "    ('Görünmez Savaşçı', 0.38609176874160767)]\n",
      "tony stark bir mühendislik dahisi tam bir playboydur kendi ülkesinde teknoloji harikası füzeler silahlar üretmektedir afganistan ’ da yeni bir füzeyi tanıtırken esir düşer yaralanır onu kaçıranlar kendileri bir füze yapmasını isterler tony bunun yerine zırhlı bir giysi yapar bunu yapmaktaki amacı zekasını kullanarak farklı bir kurtuluş yöntemini planlamaktır boş zamanlarını kadınlarına ayıran tony ’ nin hayatı artık tamamen farklı bir şekle bürünmüştür onun bununla baş edeceği esas konudur\n"
     ]
    }
   ],
   "source": [
    "def search_by_tag(tag):\n",
    "    for doc in tagged_data:\n",
    "        if doc.tags[0] == tag:\n",
    "            return \" \".join(doc.words)\n",
    "\n",
    "def return_most_similar(mov_title) -> list:\n",
    "  tokens = remove_html(remove_punctuation(remove_stop_words(make_lower_case(search_by_tag(mov_title))))).split()\n",
    "  vector = model_dbow.infer_vector(tokens)\n",
    "  most_similar = model_dbow.dv.most_similar([vector], topn=20) \n",
    "  return most_similar\n",
    "\n",
    "def return_token(mov_title) -> list:\n",
    "  tokens = remove_html(remove_punctuation(remove_stop_words(make_lower_case(search_by_tag(mov_title))))).split()\n",
    "  return tokens\n",
    "\n",
    "def return_most_similar_token(vec) -> list:\n",
    "  # len(df_final['Title']\n",
    "  most_similar = model_dbow.dv.most_similar([vec],topn=20)\n",
    "  return most_similar\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "seed_text = 'Iron Man'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)\n",
    "print(search_by_tag(seed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Dune:Çöl Gezegeni', 0.9543482065200806),\n",
      "    ('Dune', 0.45587852597236633),\n",
      "    ('House Of Re-Animator', 0.4497130811214447),\n",
      "    ('Dune: Part Two', 0.4394340515136719),\n",
      "    ('Vampir İmparatorluğu', 0.4189107418060303),\n",
      "    ('Bride Of Frankenstein', 0.4171842932701111),\n",
      "    ('Cesur Horoz', 0.41203776001930237),\n",
      "    ('Before We Vanish', 0.39964598417282104),\n",
      "    ('Cabin Fever', 0.39545121788978577),\n",
      "    ('Khers nist', 0.3914357125759125),\n",
      "    ('Avatar 5', 0.38993018865585327),\n",
      "    ('Xin long men ke zhan', 0.38973021507263184),\n",
      "    ('Kaptan Harlock', 0.388936311006546),\n",
      "    ('Resident Evil 5: İntikam', 0.38832470774650574),\n",
      "    ('Tanrının Kitabi', 0.38437628746032715),\n",
      "    ('Casuslar Köprüsü', 0.3836266100406647),\n",
      "    ('The Caine Mutiny Court-Martial', 0.3832657039165497),\n",
      "    ('28 Gün Sonra', 0.3817640542984009),\n",
      "    ('Ant-Man ve Wasp: Quantumania', 0.3809562921524048),\n",
      "    ('Star Wars: Güç Uyanıyor', 0.38010355830192566)]\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Dune:Çöl Gezegeni'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Yüzüklerin Efendisi: İki Kule', 0.9500899910926819),\n",
      "    ('Pearl Harbor', 0.47223082184791565),\n",
      "    ('Gelibolu', 0.4592815935611725),\n",
      "    ('King Kong vs. Godzilla', 0.4275374710559845),\n",
      "    ('The Lord Of The Rings: The War Of Rohirrim', 0.4228450357913971),\n",
      "    ('Hobbit: Beş Ordunun Savaşı', 0.41768234968185425),\n",
      "    ('Uzay Yolu IV', 0.4161980152130127),\n",
      "    ('Murder in the First', 0.4150218665599823),\n",
      "    ('Harry Potter ve Ateş Kadehi', 0.4138086140155792),\n",
      "    ('The Big Black', 0.41283881664276123),\n",
      "    ('Gezgin Pantolon Kardeşliği 2', 0.4121832847595215),\n",
      "    ('Pers Prensi: Zamanın Kumları', 0.40483254194259644),\n",
      "    ('The Day It Came to Earth', 0.39698025584220886),\n",
      "    ('Skinwalkers', 0.3954154849052429),\n",
      "    ('Da Vinci Şifresi', 0.3941201865673065),\n",
      "    ('Rescue Dawn', 0.39369285106658936),\n",
      "    ('Under sandet', 0.3923293650150299),\n",
      "    (\"Mortal Kombat Legends : Scorpion's Revenge\", 0.39112597703933716),\n",
      "    ('Üçkağıtçı', 0.39096465706825256),\n",
      "    ('Studio 54', 0.3909226655960083)]\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Yüzüklerin Efendisi: İki Kule'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Rashômon', 0.9476543664932251),\n",
      "    ('Françoise ou la vie conjugale', 0.5435649156570435),\n",
      "    ('İntikam Meleği/Kadın Hamlet', 0.5181604027748108),\n",
      "    ('Gölgeler ve Sis', 0.4887164831161499),\n",
      "    ('Seules Les Bêtes', 0.4784468710422516),\n",
      "    ('Deathstalker', 0.4713344871997833),\n",
      "    ('Münferit', 0.4707236588001251),\n",
      "    ('Ölüm Gemisi', 0.4622574746608734),\n",
      "    ('Parçalanma', 0.46023598313331604),\n",
      "    ('Kanlı Feryad', 0.458997517824173),\n",
      "    (\"Fatmagül'ün Suçu Ne?\", 0.4543512165546417),\n",
      "    ('Korkunç Orakçı', 0.45266059041023254),\n",
      "    ('Aşk Nöbeti', 0.4518894553184509),\n",
      "    ('Absentia', 0.45114442706108093),\n",
      "    ('Detained', 0.44956687092781067),\n",
      "    ('Kehanet; Ayasofya', 0.44840914011001587),\n",
      "    ('Yılanların Öcü', 0.44721612334251404),\n",
      "    ('De Lift', 0.44244807958602905),\n",
      "    ('Matar a un Muerto', 0.442259818315506),\n",
      "    ('The Exorcist III', 0.44154855608940125)]\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Rashômon'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   ('Titanik', 0.9488952159881592),\n",
      "    ('Dinozor', 0.4613831341266632),\n",
      "    ('Beavis and Butt-Head Do the Universe', 0.4197002351284027),\n",
      "    ('The Lord Of The Rings: The War Of Rohirrim', 0.4190202057361603),\n",
      "    ('Last Words', 0.4082288146018982),\n",
      "    ('Anchiporuno', 0.40757089853286743),\n",
      "    ('Apollo 10 1/2: Uzay Çağında Çocuk Olmak', 0.3962409496307373),\n",
      "    ('Nos Futurs', 0.39575523138046265),\n",
      "    ('İyilik ve Kötülük Okulu', 0.39204472303390503),\n",
      "    ('Spider-Man: Beyond The Spider-Verse', 0.38597914576530457),\n",
      "    ('2 Kız', 0.3858177959918976),\n",
      "    ('Frankenstein: Ölümsüzlerin Savaşı', 0.3837279677391052),\n",
      "    ('Cirque du Soleil: Worlds Away', 0.38000935316085815),\n",
      "    ('Shirin', 0.37940600514411926),\n",
      "    ('Booksmart', 0.37793421745300293),\n",
      "    ('2012', 0.37791845202445984),\n",
      "    ('Summer Of 8', 0.3772253394126892),\n",
      "    ('Vic + Flo ont vu un ours', 0.3758023977279663),\n",
      "    ('Pas ve Gol', 0.3735332190990448),\n",
      "    ('Ruh Eşim', 0.3732932507991791)]\n"
     ]
    }
   ],
   "source": [
    "seed_text = 'Titanik'\n",
    "x = return_most_similar(seed_text)\n",
    "pp.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this dict has title:embedding -> embeddings are created using that specific movie's plot\n",
    "movie_embeddings = {title: model_dbow.infer_vector(return_token(title)) for title in df_final['Title']}\n",
    "\n",
    "with open(\"movie_embed_genre.pickle\", \"wb\") as f:\n",
    "    pickle.dump(movie_embeddings, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining our embeddings and saving them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('movie_embed_desc.pickle', 'rb') as f:\n",
    "    desc = pickle.load(f)\n",
    "with open('movie_embed_genre.pickle', 'rb') as f_1:\n",
    "    other = pickle.load(f_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rank_list(embeds:dict, mov_title:str) -> list:\n",
    "\n",
    "  selected_vector = embeds[mov_title]\n",
    "\n",
    "  similarities = []\n",
    "  for movie_title, vector in embeds.items():\n",
    "      cos_sim = 1 - distance.cosine(selected_vector, vector)\n",
    "      similarities.append((movie_title, cos_sim))\n",
    "\n",
    "  ranked_movies = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "  ranked_movies = [x for x in ranked_movies if x[0] != mov_title]\n",
    "\n",
    "\n",
    "  dct = {}\n",
    "  # replacing cosine similarities with ranks\n",
    "  for index, item in enumerate(ranked_movies):\n",
    "    itemlist = list(item)\n",
    "    # itemlist[1] = index + 1\n",
    "    # item = tuple(itemlist)\n",
    "\n",
    "    # ranked_movies[index] = item\n",
    "\n",
    "    dct[itemlist[0]] = index+1\n",
    "\n",
    "\n",
    "  return dct\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    merged_dict = {}\n",
    "    for title, embedding1 in dict1.items():\n",
    "        embedding2 = dict2[title]\n",
    "        merged_embedding = (embedding1 * 0.7 + embedding2 * 0.3)\n",
    "        merged_dict[title] = merged_embedding\n",
    "    return merged_dict\n",
    "merged_dict = merge_dicts(desc, other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_title = 'Hababam Sınıfı Uyanıyor'\n",
    "closest_movies = return_rank_list(merged_dict, mov_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings/movie_embeddings.pickle\", \"wb\") as f:\n",
    "    pickle.dump(merged_dict, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import nltk\n",
    "from scipy.spatial import distance\n",
    "import re\n",
    "from random import shuffle\n",
    "import string\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import Doc2Vec    \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import ast\n",
    "import pickle\n",
    "import pprint\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('embeddings/movie_embeddings.pickle', 'rb') as f_1:\n",
    "    embeds = pickle.load(f_1)\n",
    "    \n",
    "def return_rank_list(embeds:dict, mov_title:str) -> list:\n",
    "\n",
    "  selected_vector = embeds[mov_title]\n",
    "\n",
    "  similarities = []\n",
    "  for movie_title, vector in embeds.items():\n",
    "      cos_sim = 1 - distance.cosine(selected_vector, vector)\n",
    "      similarities.append((movie_title, cos_sim))\n",
    "\n",
    "  ranked_movies = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "  ranked_movies = [x for x in ranked_movies if x[0] != mov_title]\n",
    "\n",
    "\n",
    "  dct = {}\n",
    "  # replacing cosine similarities with ranks\n",
    "  for index, item in enumerate(ranked_movies):\n",
    "    itemlist = list(item)\n",
    "    # itemlist[1] = index + 1\n",
    "    # item = tuple(itemlist)\n",
    "\n",
    "    # ranked_movies[index] = item\n",
    "\n",
    "    dct[itemlist[0]] = index+1\n",
    "\n",
    "\n",
    "  return dct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess_movie = 'Iron Man'\n",
    "# valid_movies = list(embeds.keys())\n",
    "# game = True\n",
    "# sim_movies_list = return_rank_list(embeds,guess_movie)\n",
    "# while game:\n",
    "#     user_input = input()\n",
    "#     if user_input not in valid_movies:\n",
    "#         print('I do not know this movie')\n",
    "#     else:\n",
    "#         if (user_input == guess_movie):\n",
    "#             print('You have won.')\n",
    "#             break\n",
    "#         print(f'{user_input}:{sim_movies_list[user_input]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import customtkinter\n",
    "import tkinter as tk\n",
    "guess_movie = \"Hababam Sınıfı Uyanıyor\"\n",
    "valid_movies = list(embeds.keys())\n",
    "sim_movies_list = return_rank_list(embeds,guess_movie)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tkinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\tkinter\\__init__.py\", line 1921, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\customtkinter\\windows\\widgets\\ctk_button.py\", line 553, in _clicked\n",
      "    self._command()\n",
      "  File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_11340\\45447972.py\", line 31, in check_guess\n",
      "    previous_guesses.append((user_input, sim_movies_list[user_input]))\n",
      "KeyError: 'Recep İvedik 5'\n"
     ]
    }
   ],
   "source": [
    "def main_window():\n",
    "    customtkinter.set_appearance_mode(\"dark\")\n",
    "    customtkinter.set_default_color_theme(\"dark-blue\")\n",
    "\n",
    "    root = customtkinter.CTk()\n",
    "    root.title(\"Movie Guess Game\")\n",
    "    root.geometry(\"650x450\")\n",
    "    frame = customtkinter.CTkFrame(master=root)\n",
    "    frame.pack(pady=15, padx=30, fill=\"both\",expand=True)\n",
    "\n",
    "    label = customtkinter.CTkLabel(frame, text=\"Guess the movie\", font=(\"Helvetica\", 16))\n",
    "    label.pack(pady=15, padx=30, fill=\"both\",expand=True)\n",
    "\n",
    "    guess_entry = customtkinter.CTkEntry(frame, font=(\"Helvetica\", 14))\n",
    "    guess_entry.pack(pady=10)\n",
    "\n",
    "    previous_guesses = []\n",
    "    def game_check(user_input):\n",
    "        if (user_input == guess_movie):\n",
    "            result_label.configure(text=\"You have won. You may close the window.\")\n",
    "            guess_entry.configure(state=\"disabled\")\n",
    "            guess_button.configure(state=\"disabled\")\n",
    "\n",
    "    def check_guess():\n",
    "        user_input = guess_entry.get().strip() \n",
    "        game_check(user_input)    \n",
    "        if user_input not in valid_movies:\n",
    "            result_label.configure(text=\"I do not know this movie\")\n",
    "        else:\n",
    "            game_check(user_input)\n",
    "            previous_guesses.append((user_input, sim_movies_list[user_input]))\n",
    "            previous_guesses.sort(key=lambda x: x[1], reverse=False)\n",
    "            result_label.configure(text=f\"{user_input}:{sim_movies_list[user_input]}\")\n",
    "        previous_guesses_list.delete(0, tk.END)\n",
    "        for guess in previous_guesses:\n",
    "            previous_guesses_list.insert(tk.END, f\"{guess[0]}:{guess[1]}\")\n",
    "\n",
    "    guess_button = customtkinter.CTkButton(frame, text=\"Guess\", command=check_guess, font=(\"Helvetica\", 14))\n",
    "    guess_button.pack(pady=10)\n",
    "\n",
    "    result_label = customtkinter.CTkLabel(master=frame, text=f\"\")\n",
    "    result_label.pack(pady=20)\n",
    "\n",
    "    previous_guesses_label = customtkinter.CTkLabel(frame, text=\"Previous guesses:\", font=(\"Helvetica\", 14))\n",
    "    previous_guesses_label.pack(pady=10)\n",
    "\n",
    "\n",
    "    previous_guesses_list = tk.Listbox(frame, fg = \"white\" ,bg = \"#262629\", font=(\"Helvetica\", 14))\n",
    "    previous_guesses_list.pack(pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "main_window()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_movie = \"Recep İvedik 5\"\n",
    "valid_movies = list(embeds.keys())\n",
    "sim_movies_list = return_rank_list(embeds,guess_movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_window():\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Movie Guess Game\")\n",
    "    root.geometry(\"650x450\")\n",
    "    root.config(bg='#303030')\n",
    "\n",
    "    label = tk.Label(root, text=\"Guess the movie\", bg='#303030', fg='white', font=(\"Helvetica\", 16))\n",
    "    label.pack(pady=20)\n",
    "\n",
    "    guess_entry = tk.Entry(root, bg='white', fg='#303030', font=(\"Helvetica\", 14))\n",
    "    guess_entry.pack(pady=10)\n",
    "\n",
    "    previous_guesses = []\n",
    "\n",
    "    def check_guess():\n",
    "        \n",
    "        user_input = guess_entry.get()\n",
    "        if (user_input == guess_movie):\n",
    "            result_label.config(text=\"You have won.\", bg='#303030', fg='white')\n",
    "            guess_entry.config(state=\"disabled\")\n",
    "            guess_button.config(state=\"disabled\")\n",
    "        if user_input not in valid_movies:\n",
    "            result_label.config(text=\"I do not know this movie\", bg='#303030', fg='white')\n",
    "        else:\n",
    "            previous_guesses.append((user_input, sim_movies_list[user_input]))\n",
    "            previous_guesses.sort(key=lambda x: x[1], reverse=False)\n",
    "            if (user_input == guess_movie):\n",
    "                result_label.config(text=\"You have won.\", bg='#303030', fg='white')\n",
    "                guess_entry.config(state=\"disabled\")\n",
    "                guess_button.config(state=\"disabled\")\n",
    "        previous_guesses_list.delete(0, tk.END)\n",
    "        for guess in previous_guesses:\n",
    "            previous_guesses_list.insert(tk.END, f\"{guess[0]}:{guess[1]}\")\n",
    "\n",
    "    guess_button = tk.Button(root, text=\"Guess\", command=check_guess, bg='#303030', fg='white', font=(\"Helvetica\", 14))\n",
    "    guess_button.pack(pady=10)\n",
    "\n",
    "    result_label = tk.Label(root, bg='#303030', fg='white')\n",
    "    result_label.pack(pady=20)\n",
    "\n",
    "    previous_guesses_label = tk.Label(root, text=\"Previous guesses:\", bg='#303030', fg='white', font=(\"Helvetica\", 14))\n",
    "    previous_guesses_label.pack(pady=10)\n",
    "\n",
    "    previous_guesses_list = tk.Listbox(root, bg='white', fg='#303030', font=(\"Helvetica\", 14))\n",
    "    previous_guesses_list.pack(pady=10)\n",
    "\n",
    "    root.mainloop()\n",
    "\n",
    "main_window()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "afb734500600fd355917ca529030176ea0ca205570884b88f2f6f7d791fd3fbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
